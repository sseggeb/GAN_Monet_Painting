{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31155,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I'm Something of a Painter Myself (GAN - Getting Started)\nKaggle competition for learning about Generative Adversarial Networks (GANs).\n\n**Goal:** Use a GAN to generate original images that mimic the artistic style of Claude Monet.\n\n**Task:** Build a GAN consisting of a generator and a discriminator and use it to generate 7,000 - 10,000 Monet-style images.\n\n**Data:** The main dataset contains Monet paintings and a collection of photographs.\n\n**Evaluation:** Submissions are evaluated using MiFID.  A lower score is better (<1000 required for class).  This metric measures the similarity\nbetween the statistics of the real Monet images and the generated images, while penalizing images that are too close to the training samples.\n\n**Submission:** a single images.zip file containing the 7,000-10,000 generated images, each sized 256x256x3 (RGB).\n\nThe competition overview recommends following Amy Jang's notebook that goes over the basics of loading data from TFRecods, using TPUs, and building a CycleGAN, so that's where I will start.\n\nSources: [TPU Notebook Walkthrough: Introduction to TFRecords] https://www.youtube.com/watch?v=KgjaC9VeOi8, Kaggle recommended starter notebooks and tutorial from Amy Jang is used extensively.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\n\n# Define the setup function\ndef setup_strategy():\n    try:\n        # Check if TPU is available\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Device:', tpu.master())\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        # Create a TPU distributed strategy\n        strategy = tf.distribute.TPUStrategy(tpu)\n    except:\n        # Default to CPU or single GPU if TPU initialization fails\n        strategy = tf.distribute.get_strategy()\n    \n    print('Number of replicas in sync:', strategy.num_replicas_in_sync)\n    return strategy\n\n# Execute the setup\nstrategy = setup_strategy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:02:18.496743Z","iopub.execute_input":"2025-10-28T16:02:18.496897Z","iopub.status.idle":"2025-10-28T16:02:51.490902Z","shell.execute_reply.started":"2025-10-28T16:02:18.496882Z","shell.execute_reply":"2025-10-28T16:02:51.489995Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Number of replicas in sync: 1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Load the Data from TFRecords\nTPUs need data streamlined efficiently, the data for this competition is provided in TFRecord format. Parse the records into TensorFlow datasets.","metadata":{}},{"cell_type":"code","source":"# Get the data paths\nwith strategy.scope():\n    PATH = '/kaggle/input/gan-getting-started'\n\n    # Get the files for the Monet paintings and the Photos\n    MONET_FILENAMES = tf.io.gfile.glob(str(PATH + '/monet_tfrec/*.tfrec'))\n    PHOTO_FILENAMES = tf.io.gfile.glob(str(PATH + '/photo_tfrec/*.tfrec'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:03:20.927038Z","iopub.execute_input":"2025-10-28T16:03:20.927281Z","iopub.status.idle":"2025-10-28T16:03:20.939510Z","shell.execute_reply.started":"2025-10-28T16:03:20.927256Z","shell.execute_reply":"2025-10-28T16:03:20.938723Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define data processing functions\ndef decode_image(image):\n    # Decode the image bytes to a tensor\n    image = tf.image.decode_jpeg(image, channels=3)\n    # Convert to float and normalize to [-1, 1]\n    image = tf.cast(image, tf.float32)\n    image = (image / 127.5) - 1\n    return image\n\n# Function to read a single image from the TFRecord\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image\n\n# Function to load the full dataset\ndef load_dataset(filenames):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:03:24.643976Z","iopub.execute_input":"2025-10-28T16:03:24.644233Z","iopub.status.idle":"2025-10-28T16:03:24.648466Z","shell.execute_reply.started":"2025-10-28T16:03:24.644216Z","shell.execute_reply":"2025-10-28T16:03:24.647704Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Create final datasets\nBATCH_SIZE = 1 # CycleGAN typically uses a batch size of 1\n\n# Load the datasets\nmonet_ds = load_dataset(MONET_FILENAMES).shuffle(300).batch(BATCH_SIZE).repeat()\nphoto_ds = load_dataset(PHOTO_FILENAMES).shuffle(2000).batch(BATCH_SIZE).repeat()\n\n# Zip them together for CycleGAN training\nfull_dataset = tf.data.Dataset.zip((monet_ds, photo_ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:03:32.505977Z","iopub.execute_input":"2025-10-28T16:03:32.506223Z","iopub.status.idle":"2025-10-28T16:03:32.540865Z","shell.execute_reply.started":"2025-10-28T16:03:32.506207Z","shell.execute_reply":"2025-10-28T16:03:32.539854Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Building the CycleGAN model\nThe starter notebook uses CycleGAN.  CycleGAN uses two paird GANs to learn a mapping between two image domains without paired examples.  ","metadata":{}},{"cell_type":"markdown","source":"### Generator Architecture (U-Net)\nGenerator's role is to translate an image from one domain to the other.  In this case, from a photo to a Monet Style painting.  We'll use U-Net for image-to-image translation.  The structure consists of an Encoder and a Decoder with a skip connection linking them.  \n\nThe encoder is used to capture the general features and context of the image.  It has 3 components: Conv2D, InstanceNormalization, and LeakyReLU.  The stride is set to 2 to halve the image dimensions.\n\nThe decoder is used to reconstruct the image with the translated style.  It also has 3 components: Conv2DTranspose, InstanceNormalization, and ReLU.\n\nA final Conv2D layer with a tanh activation function is used as the output layer in the normalization range of [-1,1] (the standard input range for the tanh activation).","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\nimport tensorflow as tf\n\ndef downsample(filters, size, apply_norm=True):\n    # This block performs Conv -> Norm -> Activation\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same', \n                             kernel_initializer=initializer, use_bias=False))\n    if apply_norm:\n        result.add(layers.BatchNormalization())\n    result.add(layers.LeakyReLU())\n    return result\n\ndef upsample(filters, size, apply_dropout=False):\n    # This block performs ConvTranspose -> Norm -> Activation\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same', \n                                      kernel_initializer=initializer, use_bias=False))\n    result.add(layers.BatchNormalization())\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n    result.add(layers.ReLU())\n    return result\n\n# The full Generator model definition\ndef Generator():\n    INPUT_SHAPE = (256, 256, 3)\n    inputs = layers.Input(shape=INPUT_SHAPE)\n\n    # Encoder path\n    down_stack = [\n        downsample(64, 4, apply_norm=False), # (128, 128, 64)\n        downsample(128, 4),                  # (64, 64, 128)\n        downsample(256, 4),                  # (32, 32, 256)\n        downsample(512, 4),                  # (16, 16, 512)\n        downsample(512, 4),                  # (8, 8, 512)\n        downsample(512, 4),                  # (4, 4, 512)\n        downsample(512, 4),                  # (2, 2, 512)\n        downsample(512, 4),                  # (1, 1, 512)\n    ]\n\n    # Decoder path\n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (2, 2, 512)\n        upsample(512, 4, apply_dropout=True), # (4, 4, 512)\n        upsample(512, 4, apply_dropout=True), # (8, 8, 512)\n        upsample(512, 4),                     # (16, 16, 512)\n        upsample(256, 4),                     # (32, 32, 256)\n        upsample(128, 4),                     # (64, 64, 128)\n        upsample(64, 4),                      # (128, 128, 64)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    # The final layer to output the 256x256x3 image\n    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same', \n                                  kernel_initializer=initializer, activation='tanh')\n    \n    x = inputs\n    skips = []\n    \n    # Encoder\n    for down in down_stack:\n        x = down(x)\n        skips.append(x) # Store output for skip connection\n    \n    skips = reversed(skips[:-1]) # Skip the very last (1x1) layer output\n    \n    # Decoder\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        # Apply skip connection\n        x = layers.Concatenate()([x, skip])\n\n    x = last(x) # (256, 256, 3)\n\n    return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:04:23.547817Z","iopub.execute_input":"2025-10-28T16:04:23.548155Z","iopub.status.idle":"2025-10-28T16:04:23.555574Z","shell.execute_reply.started":"2025-10-28T16:04:23.548133Z","shell.execute_reply":"2025-10-28T16:04:23.554946Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Discriminator Architecture (PatchGAN)\nThe role of the discriminator is to distinguish between real images from Monet and fake images from the generator.  PatchGAN outputs a 30x30 grid of predictions, each value in the grid corresponds to whether a small patch of the input image is real or fake.\n\nA series of Conv2D layers is used to reduce the image size tothe patch grid (30x30).  The output layer is a final Conv2D layer with a single channel output (30x30x1).  This is a patch prediction map where each pixel is the discriminator's score for that patch.","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    INPUT_SHAPE = (256, 256, 3)\n    inputs = layers.Input(shape=INPUT_SHAPE, name='input_image')\n\n    # C64 - No Norm on first layer\n    initializer = tf.random_normal_initializer(0., 0.02)\n    x = layers.Conv2D(64, 4, strides=2, padding='same', \n                      kernel_initializer=initializer, use_bias=False)(inputs)\n    x = layers.LeakyReLU(0.2)(x) # (128, 128, 64)\n\n    # C128\n    x = layers.Conv2D(128, 4, strides=2, padding='same', \n                      kernel_initializer=initializer, use_bias=False)(x)\n    x = layers.BatchNormalization()(x) \n    x = layers.LeakyReLU(0.2)(x) # (64, 64, 128)\n\n    # C256\n    x = layers.Conv2D(256, 4, strides=2, padding='same', \n                      kernel_initializer=initializer, use_bias=False)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x) # (32, 32, 256)\n\n    # C512 \n    # Stride=1  to get the 30x30 patch\n    x = layers.Conv2D(512, 4, strides=1, padding='same', \n                      kernel_initializer=initializer, use_bias=False)(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x) # (32, 32, 512)\n\n    # Output layer (the Patch)\n    # No activation here; the output will be logits, (32, 32, 1) or (30, 30, 1) depending on padding/stride\n    last = layers.Conv2D(1, 4, strides=1, padding='same', kernel_initializer=initializer)(x)\n\n    return tf.keras.Model(inputs=inputs, outputs=last)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:05:00.969667Z","iopub.execute_input":"2025-10-28T16:05:00.969942Z","iopub.status.idle":"2025-10-28T16:05:00.974894Z","shell.execute_reply.started":"2025-10-28T16:05:00.969925Z","shell.execute_reply":"2025-10-28T16:05:00.974094Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### CycleGAN Loss Functions\nCycleGAN Architecture relies on three distinct loss funcitons: Adversarial Loss, Cycle Consistency Loss, and Identity Loss. \n\nThese loss functions will be definied inside of the TPU strategy scope.","metadata":{}},{"cell_type":"markdown","source":"#### Adversarial Loss (Generator and Discriminator)\nStandard GAN loss.  Discriminator tries to classify inputs as real or fake, while the generator tries to fool the discriminator by making its fake images classified as real.","metadata":{}},{"cell_type":"code","source":"# LAMBDA is the weighting factor for the cycle consistency loss.\nLAMBDA = 10 \n\nwith strategy.scope():\n    # We use BinaryCrossentropy with from_logits=True because the Discriminator outputs raw logits (not probabilities).\n    loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n\n    def discriminator_loss(real, generated):\n        # Loss for real images (should be close to 1)\n        real_loss = loss_obj(tf.ones_like(real), real)\n        \n        # Loss for fake images (should be close to 0)\n        generated_loss = loss_obj(tf.zeros_like(generated), generated)\n        \n        # Total loss is the average of both\n        total_disc_loss = real_loss + generated_loss\n        \n        # We average over the batch and the patch (e.g., 30x30 output)\n        return tf.nn.compute_average_loss(total_disc_loss, global_batch_size=BATCH_SIZE)\n        \n    def generator_loss(generated):\n        return tf.nn.compute_average_loss(loss_obj(tf.ones_like(generated), generated), global_batch_size=BATCH_SIZE)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:05:25.442105Z","iopub.execute_input":"2025-10-28T16:05:25.442379Z","iopub.status.idle":"2025-10-28T16:05:25.446329Z","shell.execute_reply.started":"2025-10-28T16:05:25.442359Z","shell.execute_reply":"2025-10-28T16:05:25.445727Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"#### Cycle Consistency Loss (L1)\nMAE or L1 loss between original image and the image that has completed the full cycle.","metadata":{}},{"cell_type":"code","source":"def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n    # L1 loss (Mean Absolute Error) measures pixel-wise difference\n    loss = tf.reduce_mean(tf.abs(real_image - cycled_image))\n    # The loss is multiplied by LAMBDA (10) to give it a high weight\n    return LAMBDA * loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:05:34.056899Z","iopub.execute_input":"2025-10-28T16:05:34.057181Z","iopub.status.idle":"2025-10-28T16:05:34.060361Z","shell.execute_reply.started":"2025-10-28T16:05:34.057163Z","shell.execute_reply":"2025-10-28T16:05:34.059568Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"#### Identity Loss (L1)\nMAE between a real Monet image and the output of the Monet-to-Photo generator when it is fed a Monet image.  ","metadata":{}},{"cell_type":"code","source":"def identity_loss(real_image, same_image, LAMBDA):\n    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n    return 0.5 * LAMBDA * loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:05:56.170357Z","iopub.execute_input":"2025-10-28T16:05:56.170601Z","iopub.status.idle":"2025-10-28T16:05:56.173884Z","shell.execute_reply.started":"2025-10-28T16:05:56.170583Z","shell.execute_reply":"2025-10-28T16:05:56.173019Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Compiling the Full CycleGAN Model\n#### Define the optimizers \nThere are four distinct networks, they need their own optimizers.  We'll use the Adam optimizer for all of them with parameters recommended by the original CycleGAN paper.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    # Optimizers for the two Generators (G_monet and G_photo)\n    monet_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n    \n    # Optimizers for the two Discriminators (D_monet and D_photo)\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:05:58.760238Z","iopub.execute_input":"2025-10-28T16:05:58.760506Z","iopub.status.idle":"2025-10-28T16:05:58.772447Z","shell.execute_reply.started":"2025-10-28T16:05:58.760487Z","shell.execute_reply":"2025-10-28T16:05:58.771532Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"#### Assemble the Complete CycleGAN Model\nCombine all four models and the losses using tf.keras.Model","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    # G_monet: Transforms Photo (A) -> Monet (B)\n    monet_generator = Generator()\n    # G_photo: Transforms Monet (B) -> Photo (A)\n    photo_generator = Generator()\n    \n    # D_monet: Discriminates Real/Fake Monet images\n    monet_discriminator = Discriminator()\n    # D_photo: Discriminates Real/Fake Photo images\n    photo_discriminator = Discriminator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:06:02.184537Z","iopub.execute_input":"2025-10-28T16:06:02.184792Z","iopub.status.idle":"2025-10-28T16:06:02.718651Z","shell.execute_reply.started":"2025-10-28T16:06:02.184773Z","shell.execute_reply":"2025-10-28T16:06:02.717610Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"This custom CycleGAN class from Amy Jang's tutorial will manage the forward and backward passes, calculating the four primary losses (two adversarial, cycle consistency, and identity) in every training step.","metadata":{}},{"cell_type":"code","source":"class CycleGan(tf.keras.Model):\n    def __init__(self, \n                 monet_generator, \n                 photo_generator, \n                 monet_discriminator, \n                 photo_discriminator, \n                 gen_loss_fn, \n                 disc_loss_fn, \n                 cycle_loss_fn, \n                 identity_loss_fn):\n        \n        super(CycleGan, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def compile(self, \n                m_gen_optimizer, \n                p_gen_optimizer, \n                m_disc_optimizer, \n                p_disc_optimizer, \n                **kwargs):\n        \n        super(CycleGan, self).compile(**kwargs)\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        \n    @tf.function\n    def train_step(self, data):\n        # Unpack the data: one real Monet, one real Photo\n        real_monet, real_photo = data\n        \n        # All loss calculations must be done within a GradientTape context\n        with tf.GradientTape(persistent=True) as tape:\n            # Forward Pass & Generate Images\n            # Photo -> Monet\n            fake_monet = self.m_gen(real_photo, training=True)\n            # Monet -> Photo\n            fake_photo = self.p_gen(real_monet, training=True)\n            \n            # Cycle 1: Photo -> Monet -> Photo\n            cycled_photo = self.p_gen(fake_monet, training=True)\n            # Cycle 2: Monet -> Photo -> Monet\n            cycled_monet = self.m_gen(fake_photo, training=True)\n            \n            # Identity: Monet -> Monet (no change)\n            same_monet = self.m_gen(real_monet, training=True)\n            # Identity: Photo -> Photo (no change)\n            same_photo = self.p_gen(real_photo, training=True)\n            \n            # Discriminator Predictions\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            \n            disc_real_photo = self.p_disc(real_photo, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n            \n            # Calculate Losses\n            \n            # Generator Adversarial Losses (G wants to fool D)\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n            \n            # Cycle Consistency Losses\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, LAMBDA) + \\\n                               self.cycle_loss_fn(real_photo, cycled_photo, LAMBDA)\n            \n            # Identity Losses\n            total_identity_loss = self.identity_loss_fn(real_monet, same_monet, LAMBDA) + \\\n                                  self.identity_loss_fn(real_photo, same_photo, LAMBDA)\n            \n            # Total Generator Losses\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + total_identity_loss\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + total_identity_loss\n            \n            # Discriminator Losses\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Apply Gradients\n        # Calculate gradients for each network\n        monet_gen_gradients = tape.gradient(total_monet_gen_loss, self.m_gen.trainable_variables)\n        photo_gen_gradients = tape.gradient(total_photo_gen_loss, self.p_gen.trainable_variables)\n        \n        monet_disc_gradients = tape.gradient(monet_disc_loss, self.m_disc.trainable_variables)\n        photo_disc_gradients = tape.gradient(photo_disc_loss, self.p_disc.trainable_variables)\n        \n        # Apply the gradients\n        self.m_gen_optimizer.apply_gradients(zip(monet_gen_gradients, self.m_gen.trainable_variables))\n        self.p_gen_optimizer.apply_gradients(zip(photo_gen_gradients, self.p_gen.trainable_variables))\n        \n        self.m_disc_optimizer.apply_gradients(zip(monet_disc_gradients, self.m_disc.trainable_variables))\n        self.p_disc_optimizer.apply_gradients(zip(photo_disc_gradients, self.p_disc.trainable_variables))\n        \n        # Return a dictionary of loss values for monitoring\n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:06:47.084725Z","iopub.execute_input":"2025-10-28T16:06:47.085011Z","iopub.status.idle":"2025-10-28T16:06:47.093282Z","shell.execute_reply.started":"2025-10-28T16:06:47.084988Z","shell.execute_reply":"2025-10-28T16:06:47.092349Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"#### Instantiate the CycleGAN Model\n","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    # Instantiate the custom model\n    cycle_gan_model = CycleGan(\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        gen_loss_fn=generator_loss,\n        disc_loss_fn=discriminator_loss,\n        cycle_loss_fn=calc_cycle_loss,\n        identity_loss_fn=identity_loss\n    )\n\n    # Compile the model, passing the individual optimizers to the custom compile method\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:06:54.720136Z","iopub.execute_input":"2025-10-28T16:06:54.720390Z","iopub.status.idle":"2025-10-28T16:06:54.729710Z","shell.execute_reply.started":"2025-10-28T16:06:54.720372Z","shell.execute_reply":"2025-10-28T16:06:54.728783Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"## Training the Model\nUse standard Keras .fit() method, passing in the zipped dataset (full_dataset).  Training is highly resource-intensive and the number of epochs will depend on TPU time contraints and desired scores.  Typically 25-50 epochs.","metadata":{}},{"cell_type":"code","source":"# Train the model\n# The tf.data.Dataset.zip creates a data pipeline that yields (real_monet, real_photo) pairs\nEPOCHS = 25 # Start with 25 epochs\nprint(\"Starting training...\")\n\n# Fit the model to the zipped dataset\nhistory = cycle_gan_model.fit(\n    full_dataset, \n    epochs=EPOCHS,\n    steps_per_epoch=2000 # Adjust this based on your dataset size / how many steps you want per epoch\n)\n\nprint(\"Training complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T16:07:04.160089Z","iopub.execute_input":"2025-10-28T16:07:04.160344Z","iopub.status.idle":"2025-10-28T16:07:36.689322Z","shell.execute_reply.started":"2025-10-28T16:07:04.160326Z","shell.execute_reply":"2025-10-28T16:07:36.688212Z"}},"outputs":[{"name":"stdout","text":"Starting training...\nEpoch 1/25\n","output_type":"stream"},{"traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Fit the model to the zipped dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m history = \u001b[43mcycle_gan_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Adjust this based on your dataset size / how many steps you want per epoch\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete.\u001b[39m\u001b[33m\"\u001b[39m)\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:919\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    913\u001b[39m   \u001b[38;5;66;03m# If we did not create any variables the trace we have is good enough.\u001b[39;00m\n\u001b[32m    914\u001b[39m   filtered_flat_args = (\n\u001b[32m    915\u001b[39m       \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn.function_type.unpack_inputs(\n\u001b[32m    916\u001b[39m           bound_args\n\u001b[32m    917\u001b[39m       )\n\u001b[32m    918\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    920\u001b[39m \u001b[43m      \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_concrete_variable_creation_fn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfn_with_cond\u001b[39m(inner_args, inner_kwds):\n\u001b[32m    925\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Conditionally runs initialization if it's needed.\"\"\"\u001b[39;00m\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n","\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[31mKeyboardInterrupt\u001b[39m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"cell_type":"markdown","source":"## Submission Images and Zip File\nUse the best generator to create the 7,000-10,000 images.  Write to a temp folder due to file output limit from Kaggle.","metadata":{}},{"cell_type":"code","source":"import PIL\nimport numpy as np\nimport os\nimport shutil\n\n# 1. Define the temporary output directory\nOUTPUT_DIR = '../images'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# 2. Prepare the photo dataset for generation\n# We need to iterate over ALL photo images to generate 7000+ images. \n# We'll use a new dataset iteration without shuffling or repeating.\nPHOTO_COUNT = len(PHOTO_FILENAMES) * 2000 # Approximation of total photos (~7038)\n\n# Load a fresh, unbatched dataset for clean iteration. Batch size should be 1.\nphoto_ds_single = load_dataset(PHOTO_FILENAMES).batch(1) \n\n# 3. Generate and Save Images\ni = 1\n# Iterate over the entire photo dataset (ensure you iterate enough times)\nfor img in photo_ds_single:\n    # Use the Monet Generator (m_gen) to generate the painting\n    prediction = monet_generator(img, training=False)[0].numpy()\n    \n    # Denormalize the image from [-1, 1] back to [0, 255] (uint8)\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    \n    # Save the image as a JPG file\n    im = PIL.Image.fromarray(prediction)\n    im.save(os.path.join(OUTPUT_DIR, f\"{i}.jpg\"))\n    \n    i += 1\n    if i > 7000: # Ensure you meet the minimum requirement\n        break\n\n# 4. Create the final submission file\n# The output file MUST be named images.zip and contain all the .jpg files\nshutil.make_archive('/kaggle/working/images', 'zip', OUTPUT_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}