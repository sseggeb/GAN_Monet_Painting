{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I'm Something of a Painter Myself (GAN - Getting Started)\nKaggle competition for learning about Generative Adversarial Networks (GANs).\n\n**Goal:** Use a GAN to generate original images that mimic the artistic style of Claude Monet.\n\n**Task:** Build a GAN consisting of a generator and a discriminator and use it to generate 7,000 - 10,000 Monet-style images.\n\n**Data:** The main dataset contains Monet paintings and a collection of photographs.\n\n**Evaluation:** Submissions are evaluated using MiFID.  A lower score is better (<1000 required for class).  This metric measures the similarity\nbetween the statistics of the real Monet images and the generated images, while penalizing images that are too close to the training samples.\n\n**Submission:** a single images.zip file containing the 7,000-10,000 generated images, each sized 256x256x3 (RGB).\n\nThe competition overview recommends following Amy Jang's notebook that goes over the basics of loading data from TFRecods, using TPUs, and building a CycleGAN, so that's where I will start.\n\nSources: [TPU Notebook Walkthrough: Introduction to TFRecords] https://www.youtube.com/watch?v=KgjaC9VeOi8, Kaggle recommended starter notebooks and tutorial from Amy Jang is used extensively.  Recommended Readings from Module 5.","metadata":{}},{"cell_type":"markdown","source":"I was having some issues getting the TPU to work in my notebook and found a solution here: https://www.kaggle.com/code/herbison/tensorflow-v5e-8-oct-2025?scriptVersionId=267778194","metadata":{}},{"cell_type":"code","source":"# load version 2.18.0 of tensorflow-tpu\n\n!export PATH=\"${HOME}/.local/bin:${PATH}\" && uv pip install --system tensorflow-tpu==\"2.18.0\" --find-links https://storage.googleapis.com/libtpu-tf-releases/index.html","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!export PATH=\"${HOME}/.local/bin:${PATH}\" && uv pip uninstall --system jax","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nprint('TensorFlow version' + tf.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def setup_strategy():\n    # Detect and initialize TPU\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=\"local\")  \n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.TPUStrategy(tpu)\n        print(f\"Running on TPU: {tpu.master()}\")\n    except Exception as e:\n        print(f\"Could not initialize TPU: {e}\")\n        strategy = tf.distribute.get_strategy()  # Fallback to default CPU/GPU\n        print(\"Running on CPU or single GPU\")\n    \n    # Print number of available TPU cores\n    print(\"Number of devices:\", strategy.num_replicas_in_sync)\n    return strategy\n\nstrategy = setup_strategy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the Data from TFRecords\nTPUs need data streamlined efficiently, the data for this competition is provided in TFRecord format. Parse the records into TensorFlow datasets.","metadata":{}},{"cell_type":"code","source":"# Get the data paths\nwith strategy.scope():\n    PATH = '/kaggle/input/gan-getting-started'\n\n    # Get the files for the Monet paintings and the Photos\n    MONET_FILENAMES = tf.io.gfile.glob(str(PATH + '/monet_tfrec/*.tfrec'))\n    PHOTO_FILENAMES = tf.io.gfile.glob(str(PATH + '/photo_tfrec/*.tfrec'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define data processing functions\ndef decode_image(image):\n    # Decode the image bytes to a tensor\n    image = tf.image.decode_jpeg(image, channels=3)\n    # Convert to float and normalize to [-1, 1]\n    image = tf.cast(image, tf.float32)\n    image = (image / 127.5) - 1\n    return image\n\n# Function to read a single image from the TFRecord\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image\n\n# Function to load the full dataset\ndef load_dataset(filenames):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n    return dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create final datasets\nBATCH_SIZE = 1 # CycleGAN typically uses a batch size of 1\n\n# Load the datasets\nmonet_ds = load_dataset(MONET_FILENAMES).shuffle(300).batch(BATCH_SIZE).repeat()\nphoto_ds = load_dataset(PHOTO_FILENAMES).shuffle(2000).batch(BATCH_SIZE).repeat()\n\n# Zip them together for CycleGAN training\nfull_dataset = tf.data.Dataset.zip((monet_ds, photo_ds))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Building the CycleGAN model\nThe starter notebook uses CycleGAN.  CycleGAN uses two paird GANs to learn a mapping between two image domains without paired examples.  ","metadata":{}},{"cell_type":"markdown","source":"### Generator Architecture (U-Net)\nGenerator's role is to translate an image from one domain to the other.  In this case, from a photo to a Monet Style painting.  We'll use U-Net for image-to-image translation.  The structure consists of an Encoder and a Decoder with a skip connection linking them.  \n\nThe encoder is used to capture the general features and context of the image.  It has 3 components: Conv2D, InstanceNormalization, and LeakyReLU.  The stride is set to 2 to halve the image dimensions.\n\nThe decoder is used to reconstruct the image with the translated style.  It also has 3 components: Conv2DTranspose, InstanceNormalization, and ReLU.\n\nA final Conv2D layer with a tanh activation function is used as the output layer in the normalization range of [-1,1] (the standard input range for the tanh activation).","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\nimport tensorflow as tf\nfrom tensorflow.keras.layers import GroupNormalization\n\ndef apply_normalization(x):\n    return GroupNormalization(groups=1, axis=-1)(x)\n\ndef downsample(filters, size, apply_norm=True):\n    # This block performs Conv -> Norm -> Activation\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same', \n                             kernel_initializer=initializer, use_bias=False))\n    if apply_norm:\n        result.add(GroupNormalization(groups=1, axis=-1))\n    result.add(layers.LeakyReLU())\n    return result\n\ndef upsample(filters, size, apply_dropout=False):\n    # This block performs ConvTranspose -> Norm -> Activation\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    \n    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same', \n                                      kernel_initializer=initializer, use_bias=False))\n    \n    result.add(GroupNormalization(groups=1, axis=-1))\n    \n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n        \n    result.add(layers.ReLU())\n    return result\n\n# The full Generator model definition\ndef Generator():\n    INPUT_SHAPE = (256, 256, 3)\n    inputs = layers.Input(shape=INPUT_SHAPE)\n\n    # Encoder path\n    down_stack = [\n        downsample(64, 4, apply_norm=False), # (128, 128, 64)\n        downsample(128, 4),                  # (64, 64, 128)\n        downsample(256, 4),                  # (32, 32, 256)\n        downsample(512, 4),                  # (16, 16, 512)\n        downsample(512, 4),                  # (8, 8, 512)\n        downsample(512, 4),                  # (4, 4, 512)\n        downsample(512, 4),                  # (2, 2, 512)\n        downsample(512, 4),                  # (1, 1, 512)\n    ]\n\n    # Decoder path\n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (2, 2, 512)\n        upsample(512, 4, apply_dropout=True), # (4, 4, 512)\n        upsample(512, 4, apply_dropout=True), # (8, 8, 512)\n        upsample(512, 4),                     # (16, 16, 512)\n        upsample(256, 4),                     # (32, 32, 256)\n        upsample(128, 4),                     # (64, 64, 128)\n        upsample(64, 4),                      # (128, 128, 64)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    # The final layer to output the 256x256x3 image\n    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same', \n                                  kernel_initializer=initializer, activation='tanh')\n    \n    x = inputs\n    skips = []\n    \n    # Encoder\n    for down in down_stack:\n        x = down(x)\n        skips.append(x) \n    \n    skips = reversed(skips[:-1])\n    \n    # Decoder\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n\n    x = last(x) # (256, 256, 3)\n\n    return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Discriminator Architecture (PatchGAN)\nThe role of the discriminator is to distinguish between real images from Monet and fake images from the generator.  PatchGAN outputs a 30x30 grid of predictions, each value in the grid corresponds to whether a small patch of the input image is real or fake.\n\nA series of Conv2D layers is used to reduce the image size tothe patch grid (30x30).  The output layer is a final Conv2D layer with a single channel output (30x30x1).  This is a patch prediction map where each pixel is the discriminator's score for that patch.","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n    INPUT_SHAPE = (256, 256, 3)\n    inputs = layers.Input(shape=INPUT_SHAPE, name='input_image')\n    initializer = tf.random_normal_initializer(0., 0.02)\n    \n    # C64 - No Norm on first layer\n    x = layers.Conv2D(64, 4, strides=2, padding='same', \n                      kernel_initializer=initializer, use_bias=False)(inputs)\n    x = layers.LeakyReLU(0.2)(x) # (128, 128, 64)\n\n    # C128\n    x = layers.Conv2D(128, 4, strides=2, padding='same', \n                      kernel_initializer=initializer, use_bias=False)(x)\n    x = GroupNormalization(groups=1, axis=-1)(x) \n    x = layers.LeakyReLU(0.2)(x) # (64, 64, 128)\n\n    # C256\n    x = layers.Conv2D(256, 4, strides=2, padding='same', \n                      kernel_initializer=initializer, use_bias=False)(x)\n    x = GroupNormalization(groups=1, axis=-1)(x)\n    x = layers.LeakyReLU(0.2)(x) # (32, 32, 256)\n\n    # C512 \n    # Stride=1  to get the 30x30 patch\n    x = layers.Conv2D(512, 4, strides=1, padding='same', \n                      kernel_initializer=initializer, use_bias=False)(x)\n    x = GroupNormalization(groups=1, axis=-1)(x)\n    x = layers.LeakyReLU(0.2)(x) # (32, 32, 512)\n\n    # Output layer (the Patch) (30, 30, 1)\n    last = layers.Conv2D(1, 4, strides=1, padding='same', kernel_initializer=initializer)(x)\n\n    return tf.keras.Model(inputs=inputs, outputs=last)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### CycleGAN Loss Functions\nCycleGAN Architecture relies on three distinct loss funcitons: Adversarial Loss, Cycle Consistency Loss, and Identity Loss. \n\nThese loss functions will be definied inside of the TPU strategy scope.","metadata":{}},{"cell_type":"markdown","source":"#### Adversarial Loss (Generator and Discriminator)\nStandard GAN loss.  Discriminator tries to classify inputs as real or fake, while the generator tries to fool the discriminator by making its fake images classified as real.","metadata":{}},{"cell_type":"code","source":"# LAMBDA is the weighting factor for the cycle consistency loss.\nLAMBDA = 10 \n\nwith strategy.scope():\n    # We use BinaryCrossentropy with from_logits=True because the Discriminator outputs raw logits (not probabilities).\n    loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n\n    def discriminator_loss(real, generated):\n        # Loss for real images (should be close to 1)\n        real_loss = loss_obj(tf.ones_like(real), real)\n        \n        # Loss for fake images (should be close to 0)\n        generated_loss = loss_obj(tf.zeros_like(generated), generated)\n        \n        # Total loss is the average of both\n        total_disc_loss = real_loss + generated_loss\n        \n        # We average over the batch and the patch (e.g., 30x30 output)\n        return tf.nn.compute_average_loss(total_disc_loss, global_batch_size=BATCH_SIZE)\n        \n    def generator_loss(generated):\n        return tf.nn.compute_average_loss(loss_obj(tf.ones_like(generated), generated), global_batch_size=BATCH_SIZE)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Cycle Consistency Loss (L1)\nMAE or L1 loss between original image and the image that has completed the full cycle.","metadata":{}},{"cell_type":"code","source":"def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n    # L1 loss (Mean Absolute Error) measures pixel-wise difference\n    loss = tf.reduce_mean(tf.abs(real_image - cycled_image))\n    # The loss is multiplied by LAMBDA (10) to give it a high weight\n    return LAMBDA * loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Identity Loss (L1)\nMAE between a real Monet image and the output of the Monet-to-Photo generator when it is fed a Monet image.  ","metadata":{}},{"cell_type":"code","source":"def identity_loss(real_image, same_image, LAMBDA):\n    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n    return 0.5 * LAMBDA * loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Compiling the Full CycleGAN Model\n#### Define the optimizers \nThere are four distinct networks, they need their own optimizers.  We'll use the Adam optimizer for all of them with parameters recommended by the original CycleGAN paper.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    # Optimizers for the two Generators (G_monet and G_photo)\n    monet_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, beta_1=0.5)\n    \n    # Optimizers for the two Discriminators (D_monet and D_photo)\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, beta_1=0.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Assemble the Complete CycleGAN Model\nCombine all four models and the losses using tf.keras.Model","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    monet_generator = Generator()\n    photo_generator = Generator()\n    monet_discriminator = Discriminator()\n    photo_discriminator = Discriminator()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This custom CycleGAN class from Amy Jang's tutorial will manage the forward and backward passes, calculating the four primary losses (two adversarial, cycle consistency, and identity) in every training step.","metadata":{}},{"cell_type":"code","source":"class CycleGan(tf.keras.Model):\n    def __init__(self, \n                 monet_generator, \n                 photo_generator, \n                 monet_discriminator, \n                 photo_discriminator, \n                 gen_loss_fn, \n                 disc_loss_fn, \n                 cycle_loss_fn, \n                 identity_loss_fn):\n        \n        super(CycleGan, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def compile(self, \n                m_gen_optimizer, \n                p_gen_optimizer, \n                m_disc_optimizer, \n                p_disc_optimizer, \n                **kwargs):\n        \n        super(CycleGan, self).compile(**kwargs)\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        \n    @tf.function\n    def train_step(self, data):\n        # Unpack the data: one real Monet, one real Photo\n        real_monet, real_photo = data\n        \n        # All loss calculations must be done within a GradientTape context\n        with tf.GradientTape(persistent=True) as tape:\n            # Forward Pass & Generate Images\n            # Photo -> Monet\n            fake_monet = self.m_gen(real_photo, training=True)\n            # Monet -> Photo\n            fake_photo = self.p_gen(real_monet, training=True)\n            \n            # Cycle 1: Photo -> Monet -> Photo\n            cycled_photo = self.p_gen(fake_monet, training=True)\n            # Cycle 2: Monet -> Photo -> Monet\n            cycled_monet = self.m_gen(fake_photo, training=True)\n            \n            # Identity: Monet -> Monet (no change)\n            same_monet = self.m_gen(real_monet, training=True)\n            # Identity: Photo -> Photo (no change)\n            same_photo = self.p_gen(real_photo, training=True)\n            \n            # Discriminator Predictions\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            \n            disc_real_photo = self.p_disc(real_photo, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n            \n            # Calculate Losses\n            \n            # Generator Adversarial Losses (G wants to fool D)\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n            \n            # Cycle Consistency Losses\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, LAMBDA) + \\\n                               self.cycle_loss_fn(real_photo, cycled_photo, LAMBDA)\n            \n            # Identity Losses\n            total_identity_loss = self.identity_loss_fn(real_monet, same_monet, LAMBDA) + \\\n                                  self.identity_loss_fn(real_photo, same_photo, LAMBDA)\n            \n            # Total Generator Losses\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + total_identity_loss\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + total_identity_loss\n            \n            # Discriminator Losses\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Apply Gradients\n        # Calculate gradients for each network\n        monet_gen_gradients = tape.gradient(total_monet_gen_loss, self.m_gen.trainable_variables)\n        photo_gen_gradients = tape.gradient(total_photo_gen_loss, self.p_gen.trainable_variables)\n        \n        monet_disc_gradients = tape.gradient(monet_disc_loss, self.m_disc.trainable_variables)\n        photo_disc_gradients = tape.gradient(photo_disc_loss, self.p_disc.trainable_variables)\n        \n        # Apply the gradients\n        self.m_gen_optimizer.apply_gradients(zip(monet_gen_gradients, self.m_gen.trainable_variables))\n        self.p_gen_optimizer.apply_gradients(zip(photo_gen_gradients, self.p_gen.trainable_variables))\n        \n        self.m_disc_optimizer.apply_gradients(zip(monet_disc_gradients, self.m_disc.trainable_variables))\n        self.p_disc_optimizer.apply_gradients(zip(photo_disc_gradients, self.p_disc.trainable_variables))\n        \n        # Return a dictionary of loss values for monitoring\n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Instantiate the CycleGAN Model\n","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    # Instantiate the custom model\n    cycle_gan_model = CycleGan(\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        gen_loss_fn=generator_loss,\n        disc_loss_fn=discriminator_loss,\n        cycle_loss_fn=calc_cycle_loss,\n        identity_loss_fn=identity_loss\n    )\n\n    # Compile the model, passing the individual optimizers to the custom compile method\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training the Model\nUse standard Keras .fit() method, passing in the zipped dataset (full_dataset).  Training is highly resource-intensive and the number of epochs will depend on TPU time contraints and desired scores.  Typically 25-50 epochs.","metadata":{}},{"cell_type":"code","source":"# Train the model\n# The tf.data.Dataset.zip creates a data pipeline that yields (real_monet, real_photo) pairs\nEPOCHS = 5 \nprint(\"Starting training...\")\n\n# Fit the model to the zipped dataset\nhistory = cycle_gan_model.fit(\n    full_dataset, \n    epochs=EPOCHS,\n    steps_per_epoch=1000\n)\n\nprint(\"Training complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission Images and Zip File\nUse the best generator to create the 7,000-10,000 images.  Write to a temp folder due to file output limit from Kaggle.","metadata":{}},{"cell_type":"code","source":"import PIL\nimport numpy as np\nimport os\nimport shutil\n\n# Define the temporary output directory\nOUTPUT_DIR = '../images'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Prepare the photo dataset for generation\nPHOTO_COUNT = len(PHOTO_FILENAMES) * 2000 # Approximation of total photos (~7038)\n\n# Load a fresh, unbatched dataset for clean iteration.\nphoto_ds_single = load_dataset(PHOTO_FILENAMES).batch(1) \n\n# Generate and Save Images\ni = 1\n# Iterate over the entire photo dataset \nfor img in photo_ds_single:\n    # Use the Monet Generator (m_gen) to generate the painting\n    prediction = monet_generator(img, training=False)[0].numpy()\n    \n    # Denormalize the image from [-1, 1] back to [0, 255] (uint8)\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    \n    # Save the image as a JPG file\n    im = PIL.Image.fromarray(prediction)\n    im.save(os.path.join(OUTPUT_DIR, f\"{i}.jpg\"))\n    \n    i += 1\n    if i > 7500: \n        break\n\n# Create the final submission file\nshutil.make_archive('/kaggle/working/images', 'zip', OUTPUT_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}