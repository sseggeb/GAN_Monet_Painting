{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31155,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# I'm Something of a Painter Myself (GAN - Getting Started)\nKaggle competition for learning about Generative Adversarial Networks (GANs).\n\n**Goal:** Use a GAN to generate original images that mimic the artistic style of Claude Monet.\n\n**Task:** Build a GAN consisting of a generator and a discriminator and use it to generate 7,000 - 10,000 Monet-style images.\n\n**Data:** The main dataset contains Monet paintings and a collection of photographs.\n\n**Evaluation:** Submissions are evaluated using MiFID.  A lower score is better (<1000 required for class).  This metric measures the similarity\nbetween the statistics of the real Monet images and the generated images, while penalizing images that are too close to the training samples.\n\n**Submission:** a single images.zip file containing the 7,000-10,000 generated images, each sized 256x256x3 (RGB).\n\nThe competition overview recommends following Amy Jang's notebook that goes over the basics of loading data from TFRecods, using TPUs, and building a CycleGAN, so that's where I will start.\n\nSources: [TPU Notebook Walkthrough: Introduction to TFRecords] https://www.youtube.com/watch?v=KgjaC9VeOi8, Kaggle recommended starter notebooks.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom kaggle_datasets import KaggleDatasets\n\n# 1. Define the setup function\ndef setup_strategy():\n    try:\n        # Check if TPU is available\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Device:', tpu.master())\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        # Create a TPU distributed strategy\n        strategy = tf.distribute.TPUStrategy(tpu)\n    except:\n        # Default to CPU or single GPU if TPU initialization fails\n        strategy = tf.distribute.get_strategy()\n    \n    print('Number of replicas in sync:', strategy.num_replicas_in_sync)\n    return strategy\n\n# 2. Execute the setup\nstrategy = setup_strategy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:01:03.953764Z","iopub.execute_input":"2025-10-28T15:01:03.953907Z","iopub.status.idle":"2025-10-28T15:01:33.396252Z","shell.execute_reply.started":"2025-10-28T15:01:03.953891Z","shell.execute_reply":"2025-10-28T15:01:33.395361Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Number of replicas in sync: 1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Load the Data from TFRecords\nTPUs need data streamlined efficiently, the data for this competition is provided in TFRecord format. Parse the records into TensorFlow datasets.","metadata":{}},{"cell_type":"code","source":"# Get the data paths\nwith strategy.scope():\n    # Get the GCS path for the dataset\n    PATH = '/kaggle/input/gan-getting-started'\n\n    # Get the file paths for the Monet paintings and the Photos\n    MONET_FILENAMES = tf.io.gfile.glob(str(PATH + '/monet_tfrec/*.tfrec'))\n    PHOTO_FILENAMES = tf.io.gfile.glob(str(PATH + '/photo_tfrec/*.tfrec'))\n    \n    print(f\"Monet TFRecord files: {len(MONET_FILENAMES)}\")\n    print(f\"Photo TFRecord files: {len(PHOTO_FILENAMES)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:03:43.908029Z","iopub.execute_input":"2025-10-28T15:03:43.908449Z","iopub.status.idle":"2025-10-28T15:03:43.923605Z","shell.execute_reply.started":"2025-10-28T15:03:43.908420Z","shell.execute_reply":"2025-10-28T15:03:43.922872Z"}},"outputs":[{"name":"stdout","text":"Monet TFRecord files: 5\nPhoto TFRecord files: 20\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Define data processing functions\ndef decode_image(image):\n    # Decode the image bytes to a tensor\n    image = tf.image.decode_jpeg(image, channels=3)\n    # Convert to float and normalize to [-1, 1]\n    image = tf.cast(image, tf.float32)\n    image = (image / 127.5) - 1\n    return image\n\n# Function to read a single image from the TFRecord\ndef read_tfrecord(example):\n    tfrecord_format = {\n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \"image\": tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }\n    example = tf.io.parse_single_example(example, tfrecord_format)\n    image = decode_image(example['image'])\n    return image\n\n# Function to load the full dataset\ndef load_dataset(filenames):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=tf.data.AUTOTUNE)\n    return dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:03:47.078504Z","iopub.execute_input":"2025-10-28T15:03:47.078736Z","iopub.status.idle":"2025-10-28T15:03:47.082945Z","shell.execute_reply.started":"2025-10-28T15:03:47.078720Z","shell.execute_reply":"2025-10-28T15:03:47.082206Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Create final datasets\nBATCH_SIZE = 1 # CycleGAN typically uses a batch size of 1\n\n# Load the datasets\nmonet_ds = load_dataset(MONET_FILENAMES).shuffle(300).batch(BATCH_SIZE).repeat()\nphoto_ds = load_dataset(PHOTO_FILENAMES).shuffle(2000).batch(BATCH_SIZE).repeat()\n\n# Zip them together for CycleGAN training\nfull_dataset = tf.data.Dataset.zip((monet_ds, photo_ds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:04:24.726738Z","iopub.execute_input":"2025-10-28T15:04:24.727002Z","iopub.status.idle":"2025-10-28T15:04:24.761695Z","shell.execute_reply.started":"2025-10-28T15:04:24.726986Z","shell.execute_reply":"2025-10-28T15:04:24.760827Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Building the CycleGAN model\nThe starter notebook uses CycleGAN.  CycleGAN uses two paird GANs to learn a mapping between two image domains without paired examples.  ","metadata":{}},{"cell_type":"markdown","source":"### Generator Architecture (U-Net)\nGenerator's role is to translate an image from one domain to the other.  In this case, from a photo to a Monet Style painting.  We'll use U-Net for image-to-image translation.  The structure consists of an Encoder and a Decoder with a skip connection linking them.  \n\nThe encoder is used to capture the general features and context of the image.  It has 3 components: Conv2D, InstanceNormalization, and LeakyReLU.  The stride is set to 2 to halve the image dimensions.\n\nThe decoder is used to reconstruct the image with the translated style.  It also has 3 components: Conv2DTranspose, InstanceNormalization, and ReLU.\n\nA final Conv2D layer with a tanh activation function is used as the output layer in the normalization range of [-1,1] (the standard input range for the tanh activation).","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras import layers\nimport tensorflow as tf\n\n# Using a standard function for InstanceNormalization from TF-Addons \n# (or a custom/GroupNormalization layer if TFA is avoided)\n# For simplicity, we'll use a placeholder for now, often TCN's InstanceNormalization is used\n# If using a standard Kaggle environment, you may need to install/import tensorflow_addons (tfa)\n# import tensorflow_addons as tfa \n# InstanceNormalization = tfa.layers.InstanceNormalization\n\n# (Placeholder function for the downsample block)\ndef downsample(filters, size, apply_norm=True):\n    # This block performs Conv -> Norm -> Activation\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same', \n                             kernel_initializer=initializer, use_bias=False))\n    if apply_norm:\n        # result.add(InstanceNormalization(gamma_initializer=initializer)) # Use if TFA is available\n        result.add(layers.BatchNormalization()) # Fallback for illustration\n    result.add(layers.LeakyReLU())\n    return result\n\n# (Placeholder function for the upsample block)\ndef upsample(filters, size, apply_dropout=False):\n    # This block performs ConvTranspose -> Norm -> Activation\n    initializer = tf.random_normal_initializer(0., 0.02)\n    result = tf.keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same', \n                                      kernel_initializer=initializer, use_bias=False))\n    # result.add(InstanceNormalization(gamma_initializer=initializer)) # Use if TFA is available\n    result.add(layers.BatchNormalization()) # Fallback for illustration\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n    result.add(layers.ReLU())\n    return result\n\n# The full Generator model definition\ndef Generator():\n    INPUT_SHAPE = (256, 256, 3)\n    inputs = layers.Input(shape=INPUT_SHAPE)\n\n    # Downsampling (Encoder) path\n    down_stack = [\n        downsample(64, 4, apply_norm=False), # (128, 128, 64)\n        downsample(128, 4),                  # (64, 64, 128)\n        downsample(256, 4),                  # (32, 32, 256)\n        downsample(512, 4),                  # (16, 16, 512)\n        downsample(512, 4),                  # (8, 8, 512)\n        downsample(512, 4),                  # (4, 4, 512)\n        downsample(512, 4),                  # (2, 2, 512)\n        downsample(512, 4),                  # (1, 1, 512)\n    ]\n\n    # Upsampling (Decoder) path\n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (2, 2, 512)\n        upsample(512, 4, apply_dropout=True), # (4, 4, 512)\n        upsample(512, 4, apply_dropout=True), # (8, 8, 512)\n        upsample(512, 4),                     # (16, 16, 512)\n        upsample(256, 4),                     # (32, 32, 256)\n        upsample(128, 4),                     # (64, 64, 128)\n        upsample(64, 4),                      # (128, 128, 64)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    # The final layer to output the 256x256x3 image\n    last = layers.Conv2DTranspose(3, 4, strides=2, padding='same', \n                                  kernel_initializer=initializer, activation='tanh')\n    \n    x = inputs\n    skips = []\n    \n    # Encoder\n    for down in down_stack:\n        x = down(x)\n        skips.append(x) # Store output for skip connection\n    \n    skips = reversed(skips[:-1]) # Skip the very last (1x1) layer output\n    \n    # Decoder\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        # Apply skip connection\n        x = layers.Concatenate()([x, skip])\n\n    x = last(x) # (256, 256, 3)\n\n    return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T15:11:18.524769Z","iopub.execute_input":"2025-10-28T15:11:18.525219Z","iopub.status.idle":"2025-10-28T15:11:18.533039Z","shell.execute_reply.started":"2025-10-28T15:11:18.525203Z","shell.execute_reply":"2025-10-28T15:11:18.532278Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Discriminator Architecture (PatchGAN)","metadata":{}}]}